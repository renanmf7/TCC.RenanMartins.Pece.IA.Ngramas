{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating suggestions for writing source code in C# language based on NLP.\n",
    "\n",
    "\n",
    "## N-Gram approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook was created and adapted for the work of generating suggestions using some ideas and codes as reference the notebook of the author \"Saurav Mangeshkar\" available at: \n",
    "https://www.kaggle.com/sauravmaheshkar/auto-completion-using-n-gram-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import ntpath   \n",
    "from chardet import detect\n",
    "import nltk\n",
    "import re\n",
    "import h5py\n",
    "import numpy as np\n",
    "from toolz import unique\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(title, message = None, new_line = False):\n",
    "    \"\"\"\n",
    "    Description: Function to print info on screen\n",
    "    :param title: Message title,\n",
    "    :param message: Message to print,\n",
    "    :param new_line: Indicates whether the first message will start with a line break or not.\n",
    "    \n",
    "    :return: void.\n",
    "    \"\"\"\n",
    "    \n",
    "    if new_line:\n",
    "        print('\\n')\n",
    "    \n",
    "    print(\"####################################\")\n",
    "    print(title)\n",
    "    print(\"####################################\")\n",
    "    \n",
    "    if message:\n",
    "        print(\"%s\\n\" % (message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_of_numbers_from_string(str):\n",
    "    \"\"\"\n",
    "    Description: Function to extract all the sequence of numbers from the given string.\n",
    "    :param str: String to extract sequence of numbers.\n",
    "    \n",
    "    :return - Type(Array): Array with sequence of numbers.\n",
    "    \"\"\"\n",
    "    \n",
    "    array_numbers = re.findall(r'[0-9]+', str)\n",
    "    \n",
    "    return array_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sequence_of_numbers_for_mask(str_to_replace, \n",
    "                                         array_sequence_numbers_to_search, \n",
    "                                         mask_to_replace):\n",
    "    \"\"\"\n",
    "    Description: Function to replace sequence of numbers for specific mask.\n",
    "    :param str_to_replace: String to replace sequence of numbers,\n",
    "    :param array_sequence_numbers_to_search: Sequence numbers to search for,\n",
    "    :param mask_to_replace: Mask to replace each sequence.\n",
    "    \n",
    "    :return - Type(String): String with sequence of numbers replaced by mask.\n",
    "    \"\"\"\n",
    "    \n",
    "    for number_sequence in array_sequence_numbers_to_search:\n",
    "        str_to_replace = re.sub(str(number_sequence), mask_to_replace, str_to_replace, 1)\n",
    "\n",
    "    return str_to_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_type(file):\n",
    "    \"\"\"\n",
    "    Description: Function to retrieve enconding type of file.\n",
    "    :param file: File to get enconding.\n",
    "    \n",
    "    :return - Type(String): String with enconding type of file.\n",
    "    \"\"\"\n",
    "        \n",
    "    with open(file, 'rb') as f:\n",
    "        rawdata = f.read()\n",
    "    return detect(rawdata)['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_enconding(source_file, enconding):\n",
    "    \"\"\"\n",
    "    Description: Function to change enconding of file.\n",
    "    :param source_file: File to change enconding,\n",
    "    :param enconding: Enconding to replace in source_file.\n",
    "    \n",
    "    :return: void.\n",
    "    \"\"\"\n",
    "    \n",
    "    from_codec = get_encoding_type(source_file)\n",
    "    \n",
    "    try: \n",
    "        target_file = source_file.replace(ntpath.basename(source_file), \n",
    "                                      \"123%s\" % (ntpath.basename(source_file))) \n",
    "        \n",
    "        with open(source_file, \n",
    "                  'r', \n",
    "                  encoding=from_codec) as f, open(target_file, \n",
    "                                                  'w', \n",
    "                                                  encoding=enconding) as e:\n",
    "                text = f.read()\n",
    "                e.write(text)\n",
    "                f.close()\n",
    "\n",
    "        os.remove(source_file) \n",
    "        os.rename(target_file, source_file) \n",
    "        \n",
    "    except UnicodeDecodeError:\n",
    "        print(\"Decode error for file: '%s'\" % (source_file))\n",
    "    except UnicodeEncodeError:\n",
    "        print(\"Encode error for file: '%s'\" % (source_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(list_to_flatten):\n",
    "    \"\"\"\n",
    "    Description: Function to flatten the given list.\n",
    "    :param list_to_flatten: List to flatten.\n",
    "    \n",
    "    :returns - Type(List): Flat list.\n",
    "    \"\"\"   \n",
    "    \n",
    "    return [f for child_list in list_to_flatten for f in child_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_items_from_list(list_to_remove_duplicates):\n",
    "    \"\"\"\n",
    "    Description: Function to remove duplicate itens from given list.\n",
    "    :param list_to_remove_duplicates: List to remove duplicates.\n",
    "    \n",
    "    :returns - Type(List): List without duplicates.\n",
    "    \"\"\"  \n",
    "    \n",
    "    return list(map(list, unique(map(tuple, list_to_remove_duplicates))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read C# repository functions.\n",
    "\n",
    "#### Filter C# class files from root repository downladed from: https://github.com/dotnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_c_sharp_complete_file_names_for_each_class(root_directory):\n",
    "    \"\"\"\n",
    "    Description: Function to get all complete name of files with extension \".cs\" (C# class).\n",
    "    :param root_directory: Root directory of files.\n",
    "    \n",
    "    :return - Type(List): List with all file names of C# repository.\n",
    "    \"\"\"\n",
    "    \n",
    "    C_SHARP_CLASS_FILE_EXTENSION = \".cs\"\n",
    "    \n",
    "    # List with complete path for all C# files.\n",
    "    complete_name_of_files = []\n",
    "\n",
    "    # Loop for all files with C# class extension.\n",
    "    for root, dirs, files in os.walk(root_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(C_SHARP_CLASS_FILE_EXTENSION):\n",
    "                # Append the file name to the list\n",
    "                complete_name_of_files.append(os.path.join(root, file))\n",
    "    \n",
    "    return complete_name_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_for_each_file(complete_name_of_files):\n",
    "    \"\"\"\n",
    "    Description: Function to get content of each source code file.\n",
    "    :param complete_name_of_files: List with name of each file downladed from repository.\n",
    "    \n",
    "    :return - Type(List): Corpus with all C# source code.\n",
    "    \"\"\"\n",
    "    \n",
    "    c_sharp_code_corpus = []\n",
    "\n",
    "    for file_name in complete_name_of_files:\n",
    "        try:\n",
    "            with open(file_name, \"r\", encoding=\"utf8\") as physical_file:\n",
    "                c_sharp_code_corpus.append(physical_file.read())\n",
    "                physical_file.close()\n",
    "        except:\n",
    "            change_enconding(file_name)\n",
    "            \n",
    "    return c_sharp_code_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_code_to_tokens(source_code):\n",
    "    \"\"\"\n",
    "    Description: Function to make pre-processing in source code and tokenize words.\n",
    "    :param source_code: Source code to pre-processing.\n",
    "    \n",
    "    :returns - Type(List): List of tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constant to replace numbers in tokens.\n",
    "    MASK_NUMBERS = \"|mask_number|\"\n",
    "    \n",
    "    # Split by new line character.\n",
    "    code_sentences = source_code.split('\\n')\n",
    "    \n",
    "    # Remove leading and trailing spaces.\n",
    "    code_sentences = [c.strip() for c in code_sentences]\n",
    "    \n",
    "    # Drop empty sentences.\n",
    "    code_sentences = [c for c in code_sentences if len(c) > 0]\n",
    "    \n",
    "    # Empty list to hold tokens after ntlk process.\n",
    "    tokens = []\n",
    "    \n",
    "    # Iterate through code sentences.\n",
    "    for piece_of_code  in code_sentences:\n",
    "        # Convert to a list of words.\n",
    "        token = nltk.word_tokenize(piece_of_code)\n",
    "        \n",
    "        # Replace sequence of numbers to mask.\n",
    "        for i in range(len(token)):\n",
    "            token[i] = replace_sequence_of_numbers_for_mask(\n",
    "                            token[i],\n",
    "                            get_sequence_of_numbers_from_string(token[i]),\n",
    "                            MASK_NUMBERS)\n",
    "            \n",
    "        tokens.append(token)\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_all_files(c_sharp_code_corpus, first_x_corpus = 0):\n",
    "    \"\"\"\n",
    "    Description: Function to tokenize all files.\n",
    "    :param c_sharp_code_corpus: Complete list of C# corpus (Source code).\n",
    "    :param first_x_corpus: Option to tokenize only the first X elements. Default: 0 - Tokenize all files.\n",
    "    \n",
    "    :returns - Type(List): List of tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    corpus_copy = c_sharp_code_corpus[:]\n",
    "    \n",
    "    if first_x_corpus > 0:\n",
    "        corpus_copy = corpus_copy[:first_x_corpus]\n",
    "        \n",
    "    for corpus in corpus_copy:\n",
    "        tokens.append(preprocess_code_to_tokens(corpus))\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_the_words_for_code(code_tokens):\n",
    "    \"\"\"\n",
    "    Description: Function to count words for source codes.\n",
    "    :param code_tokens: Tokens of all source repository.\n",
    "\n",
    "    :returns - (Dictionary): Dictionary with words count { Key - \"Word\", Value = Count }.\n",
    "    \"\"\"\n",
    "    \n",
    "    code_counts = {}\n",
    "\n",
    "    for code_token in code_tokens: \n",
    "        for token in code_token:\n",
    "            for token_aux in token:\n",
    "                if token_aux not in code_counts.keys():\n",
    "                    code_counts[token_aux] = 1\n",
    "                else:\n",
    "                    code_counts[token_aux] += 1 \n",
    "            \n",
    "    return code_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_out_of_code_vocabulary(tokens, count_threshold):\n",
    "    \"\"\"\n",
    "    Description: Function to create a dictionary of words (piece of code) that are not present in\n",
    "    current corpus.\n",
    "    :param tokens: List of tokens.\n",
    "    :param count_threshold: Limit of words to add in closed dictionary.\n",
    "    \n",
    "    :returns - Type(List): Closed vocabulary.\n",
    "    \"\"\"\n",
    "        \n",
    "    closed_vocabulary = []\n",
    "\n",
    "    words_count = count_the_words_for_code(tokens)\n",
    "    \n",
    "    for word, count in words_count.items():\n",
    "        if count >= count_threshold :\n",
    "          closed_vocabulary.append(word)\n",
    "\n",
    "    return closed_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_tokenize(tokens, vocabulary, unknown_token = \"<unk>\"):\n",
    "    \"\"\"\n",
    "    Description: Function to append list of tokens with unknown words (piece of code).\n",
    "    :param tokens: List of tokens,\n",
    "    :param vocabulary: Vocabulary of code,\n",
    "    :param unknown_token: Unknown token. Default: <unk>\n",
    "    \n",
    "    :returns - Type(List): List of tokens with new unknown tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary = set(vocabulary)\n",
    "    \n",
    "    new_tokenized_sentences = []\n",
    "    \n",
    "    for sentence in tokens:\n",
    "        new_sentence = []\n",
    "        \n",
    "        for token in sentence:\n",
    "            for token_aux in token:\n",
    "                if token_aux in vocabulary:\n",
    "                    new_sentence.append(token_aux)\n",
    "                else:\n",
    "                    new_sentence.append(unknown_token)\n",
    "\n",
    "        new_tokenized_sentences.append(new_sentence)\n",
    "    \n",
    "    return new_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_vocabulary_and_unknown(tokens, count_threshold):\n",
    "    \"\"\"\n",
    "    Description: Function to process vocabulary and unknown tokens.\n",
    "    :param tokens: List of tokens,\n",
    "    :param count_threshold: Limit do define wether some word is unknown or not.\n",
    "    \n",
    "    :returns - Type(List, List): Tokens list and Vocabulary list.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Auxiliar tokens to copy\n",
    "    tokens_aux = tokens[:]\n",
    "    \n",
    "    # Get closed Vocabulary\n",
    "    vocabulary = handling_out_of_code_vocabulary(tokens_aux, count_threshold)\n",
    "\n",
    "    # Updated training dataset\n",
    "    new_token_data = unknown_tokenize(tokens_aux, vocabulary)\n",
    "\n",
    "    return new_token_data, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(tokens, ngrams_number, start_token_delimiter = \"<s>\", end_token_delimiter = \"<e>\"):\n",
    "    \"\"\"\n",
    "    Description: Function to count n-grams.\n",
    "    :param tokens: List of tokens,\n",
    "    :param ngrams_number: Number of n-grams,\n",
    "    :param start_token_delimiter: Start token delimiter,\n",
    "    :param end_token_delimiter: End token delimiter.\n",
    "    \n",
    "    :returns - Type(Dictionary): Dictionary with n-grams.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_grams = {}\n",
    "\n",
    "    for sentence in tokens:\n",
    "        sentence = [start_token_delimiter]*ngrams_number + sentence + [end_token_delimiter]\n",
    "\n",
    "        sentence = tuple(sentence)\n",
    "\n",
    "        m = len(sentence) if ngrams_number==1 else len(sentence)-1\n",
    "\n",
    "        for i in range(m):\n",
    "          n_gram = sentence[i:i+ngrams_number]\n",
    "\n",
    "          if n_gram in n_grams.keys():\n",
    "            n_grams[n_gram] += 1\n",
    "          else:\n",
    "            n_grams[n_gram] = 1\n",
    "\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_for_single_word(word, \n",
    "                         previous_n_gram, \n",
    "                         n_gram_counts, \n",
    "                         nplus1_gram_counts, \n",
    "                         vocabulary_size, \n",
    "                         k = 1.0):\n",
    "    \"\"\"\n",
    "    Description: Function to calculate probability of a single word.\n",
    "    :param word: Word to calculate probability,\n",
    "    :param nplus1_gram_counts: n-grams count to plus one,\n",
    "    :param vocabulary_size: Vocabulary size,\n",
    "    :param k: k constant to calculate.\n",
    "    \n",
    "    :returns - Type(Float): Probability of a single word.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the previous_n_gram into a tuple \n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "    # Calculating the count, if exists from our freq dictionary otherwise zero\n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "\n",
    "    # The Denominator\n",
    "    denom = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "    # previous n-gram plus the current word as a tuple\n",
    "    nplus1_gram = previous_n_gram + (word,)\n",
    "\n",
    "    # Calculating the nplus1 count, if exists from our freq dictionary otherwise zero \n",
    "    nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n",
    "\n",
    "    # Numerator\n",
    "    num = nplus1_gram_count + k\n",
    "\n",
    "    # Final Fraction\n",
    "    prob = num / denom\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs(previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0) -> 'dict':\n",
    "    \n",
    "    \"\"\"\n",
    "    Description: Function to calculate probability of next n-gram.\n",
    "    :param previous_n_gram: Word to calculate probability,\n",
    "    :param n_gram_counts: Number of n-grams,\n",
    "    :param nplus1_gram_counts: n-grams count to plus one,\n",
    "    :param vocabulary: Vocabulary,\n",
    "    :param k: k constant to calculate.\n",
    "    \n",
    "    :returns - Type(Float): Probability of next n-gram.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to Tuple\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "    # Add end and unknown tokens to the vocabulary\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "\n",
    "    # Calculate the size of the vocabulary\n",
    "    vocabulary_size = len(vocabulary)\n",
    "\n",
    "    # Empty dict for probabilites\n",
    "    probabilities = {}\n",
    "\n",
    "    # Iterate over words \n",
    "    for word in vocabulary:\n",
    "\n",
    "        # Calculate probability\n",
    "        probability = prob_for_single_word(word, previous_n_gram, \n",
    "                                               n_gram_counts, nplus1_gram_counts, \n",
    "                                               vocabulary_size, k=k)\n",
    "        # Create mapping: word -> probability\n",
    "        probabilities[word] = probability\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_complete(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "    \"\"\"\n",
    "    Description: Function to complete previous words.\n",
    "    :param previous_tokens: Previous token,\n",
    "    :param n_gram_counts: Number of n-grams,\n",
    "    :param nplus1_gram_counts: n-grams count to plus one,\n",
    "    :param vocabulary: Vocabulary,\n",
    "    :param k: k constant to calculate,\n",
    "    :param start_with: Filter to start with token.\n",
    "    \n",
    "    :returns - Type(String, Float): Next token and probability.\n",
    "    \"\"\"\n",
    "    \n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "\n",
    "    # most recent 'n' words\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "\n",
    "    # Calculate probabilty for all words\n",
    "    probabilities = probs(previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary, k=k)\n",
    "\n",
    "    # Intialize the suggestion and max probability\n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "\n",
    "    # Iterate over all words and probabilites, returning the max.\n",
    "    # We also add a check if the start_with parameter is provided\n",
    "    for word, prob in probabilities.items():\n",
    "\n",
    "        if start_with != None: \n",
    "\n",
    "            if not word.startswith(start_with):\n",
    "                continue \n",
    "\n",
    "        if prob > max_prob: \n",
    "\n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "\n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "    \"\"\"\n",
    "    Description: Function to get suggestions.\n",
    "    :param previous_tokens: Previous token,\n",
    "    :param n_gram_counts_list: Number of n-grams,\n",
    "    :param vocabulary: Vocabulary,\n",
    "    :param k: k constant to calculate,\n",
    "    :param start_with: Filter to start with token.\n",
    "    \n",
    "    :returns - Type(String, Float): Next token and probability.\n",
    "    \"\"\"\n",
    "    \n",
    "    # See how many models we have\n",
    "    count = len(n_gram_counts_list)\n",
    "    \n",
    "    # Empty list for suggestions\n",
    "    suggestions = []\n",
    "    \n",
    "    # IMP: Earlier \"-1\"\n",
    "    \n",
    "    # Loop over counts\n",
    "    for i in range(count-1):\n",
    "        \n",
    "        # get n and nplus1 counts\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        nplus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        # get suggestions \n",
    "        suggestion = auto_complete(previous_tokens, n_gram_counts,\n",
    "                                    nplus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        # Append to list\n",
    "        suggestions.append(suggestion)\n",
    "        \n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################\n",
      "First 10 files:\n",
      "####################################\n",
      "ProcessResult.cs\n",
      "ProcessUtil.cs\n",
      "Program.cs\n",
      "RunTestsOptions.cs\n",
      "TestRunner.cs\n",
      "Program.cs\n",
      "CreateFrameworkListFile.cs\n",
      "DownloadFile.cs\n",
      "FileUtilities.cs\n",
      "GenerateGuid.cs\n",
      "\n",
      "\n",
      "####################################\n",
      "Number of files for N-grams:\n",
      "####################################\n",
      "57844 files.\n"
     ]
    }
   ],
   "source": [
    "# Define constants.\n",
    "ROOT_DIRECTORY = \"D:\\DsTCC\"\n",
    "\n",
    "# Get all file names.\n",
    "complete_file_names = get_all_c_sharp_complete_file_names_for_each_class(ROOT_DIRECTORY)\n",
    "\n",
    "# Print first 10 files.\n",
    "print_info(\"First 10 files:\")\n",
    "\n",
    "for file_name in complete_file_names[:10]:\n",
    "    print(ntpath.basename(file_name))\n",
    "\n",
    "# Print total number of files.\n",
    "print_info(\"Number of files for N-grams:\", new_line=True)\n",
    "print(\"%s files.\" % (len(complete_file_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get source code of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################\n",
      "Source code of first C# file class:\n",
      "####################################\n",
      "// Licensed to the .NET Foundation under one or more agreements.\n",
      "// The .NET Foundation licenses this file to you under the MIT license.\n",
      "// See the LICENSE file in the project root for more information.\n",
      "\n",
      "namespace RunTests\n",
      "{\n",
      "    public class ProcessResult\n",
      "    {\n",
      "        public ProcessResult(string standardOutput, string standardError, int exitCode)\n",
      "        {\n",
      "            StandardOutput = standardOutput;\n",
      "            StandardError = standardError;\n",
      "            ExitCode = exitCode;\n",
      "        }\n",
      "\n",
      "        public string StandardOutput { get; }\n",
      "        public string StandardError { get; }\n",
      "        public int ExitCode { get; }\n",
      "    }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_sharp_code_corpus = get_content_for_each_file(complete_file_names)\n",
    "print_info(\"Source code of first C# file class:\", c_sharp_code_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = tokenize_all_files(c_sharp_code_corpus, first_x_corpus=0)\n",
    "\n",
    "print_info(\"First 50 tokens:\")\n",
    "for token in flatten_list(tokens)[:50]:\n",
    "    print(token) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get tokens and vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 3\n",
    "new_data_tokens, vocabulary = processing_vocabulary_and_unknown(tokens, min_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get n-grams count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 3):\n",
    "    n_model_counts = count_n_grams(new_data_tokens, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"ReadFile\", \"(\"]\n",
    "suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "display(suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
